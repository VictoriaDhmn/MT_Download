{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to download MotionTag data and upload it to the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Created by Victoria Dahmen - Project Mobilität.Leben - last updated on 20th March 2024*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions (marked with **'TODO'**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview:\n",
    "\n",
    "# --- DOWNLOAD --------------------------------------------------------------\n",
    "# Step 0: set up the environment i.e. all filepaths\n",
    "# Step 1: download the WP and SL data (WP=waypoints, SL=storyline)       \n",
    "\n",
    "# --- PROCESS ---------------------------------------------------------------\n",
    "# Step 2: convert the gzip files to csv                                  \n",
    "# Step 3: move .gz files from 'new' folder to 'gz' folder (as backups; much smaller than csv files)   \n",
    "# Step 4: split the WP csv files into separate days                      \n",
    "\n",
    "# --- UPLOAD ----------------------------------------------------------------\n",
    "# Step 5: upload WP to PostgreSQL database, then delete the WP csv files                                                                                  \n",
    "# Step 6: upload SL to PostgreSQL database, then delete the SL csv files                           \n",
    "# Step 7: run processing pipeline\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Folder structure:\n",
    "\n",
    "# MT_MoLe_tmp (root)\n",
    "# ├── new_waypoints         (new downloaded waypoints - first as gz, then as csv, then deleted)\n",
    "# ├── new_storylines        (new downloaded storylines - first as gz, then as csv,  then deleted)\n",
    "# ├── gz_waypoints          (move waypoints gz files here as a backup)\n",
    "# ├── gz_storylines         (move storylines gz files here as a backup)\n",
    "# this file\n",
    "# MotionTagConfigKey.txt\n",
    "# C:\\...\\myDatabasePassword.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME FUNCTIONS\n",
    "\n",
    "import jwt\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import psycopg2 # pip install psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# not required for steps 1-4\n",
    "def setUpDBConnection(user, myDBPasswordPath, host='XX.XXX.XX.XXX', dbname='mobilitaetleben'):\n",
    "    # 1. set password\n",
    "    with open(myDBPasswordPath) as f:\n",
    "        mypassword = f.read()\n",
    "    # 2. open connection\n",
    "    try: \n",
    "        conn = psycopg2.connect(host=host, dbname=dbname, user=user, password=mypassword)\n",
    "    except psycopg2.Error as e: \n",
    "        print(\"Error: Could not make connection to the Postgres database\")\n",
    "        print(e)\n",
    "    # 3. make cursor\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Make sure this data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WP = waypoints, SL = storylines\n",
    "\n",
    "# TODO: create folder \"MT_MoLe_tmp\" next to this file\n",
    "# TODO: in \"MT_MoLe_tmp\", create subfolders \"new_waypoints\", \"new_storylines\", \"gz_waypoints\", \"gz_storylines\"\n",
    "# TODO: update all the file paths below, as applicable\n",
    "\n",
    "# set path to text file that contains the MotionTagConfigKey\n",
    "MTConfigFile = r\"MotionTagConfigKey.txt\"\n",
    "\n",
    "# path to save the downloaded files to)\n",
    "path_WP_new = r\"MT_MoLe_tmp\\new_waypoints\" \n",
    "path_SL_new = r\"MT_MoLe_tmp\\new_storylines\" \n",
    "\n",
    "# path to save the backup files to\n",
    "path_WP_gz = r\"MT_MoLe_tmp\\gz_waypoints\" \n",
    "path_SL_gz = r\"MT_MoLe_tmp\\gz_storylines\" \n",
    "\n",
    "# Database connection information\n",
    "myDBPasswordPath = r\"C:\\Users\\...\\myDBPassword.txt\"\n",
    "myDBUsername = \"postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last storyline date:  2024-01-18\n",
      "last waypoint date:  2023-12-08 23:59:59\n",
      "\n",
      "NOTE: Use current last day as new first day for storyline (not for waypoints)\n"
     ]
    }
   ],
   "source": [
    "# get last storyline and waypoints date\n",
    "conn, cursor = setUpDBConnection(myDBUsername, myDBPasswordPath)\n",
    "query = \"select max(started_on) from storyline;\"\n",
    "cursor.execute(query)\n",
    "last_storyline_date = cursor.fetchone()[0]\n",
    "query = \"select max(tracked_at) from waypoints;\"\n",
    "cursor.execute(query)\n",
    "last_waypoint_date = cursor.fetchone()[0]\n",
    "print(\"last storyline date: \", last_storyline_date)\n",
    "print(\"last waypoint date: \", last_waypoint_date)\n",
    "print('\\nNOTE: Use current last day as new first day for storyline (not for waypoints)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAYPOINT DATA\n",
    " \n",
    "# TODO: set start and end date for the waypoints data\n",
    "start_date = pd.to_datetime('2024-01-03') # included\n",
    "end_date = pd.to_datetime('2024-01-20') # included\n",
    "# TODO: number of days per file\n",
    "offset = 1\n",
    "\n",
    "def generateWaypointsFilenames(start_date,end_date,offset):\n",
    "    fileURLs = []\n",
    "    for sd in pd.date_range(start_date, end_date, freq=f'{offset}D'):\n",
    "        # sample file name: Waypoints.TUM-ML.2023-06-01--2023-06-07.csv.gz\n",
    "        filename = 'Waypoints.TUM-ML.' + str(sd.date()) + '--' + str((sd + pd.DateOffset(days=offset-1)).date()) + '.csv.gz'\n",
    "        start_date = start_date + pd.DateOffset(days=offset)\n",
    "        fileURLs.append(filename)\n",
    "    return fileURLs\n",
    "\n",
    "def downloadData(fileURLs,path,MTConfigFile):\n",
    "    with open(MTConfigFile, 'r') as file:\n",
    "        MotionTagConfigKey = file.read()\n",
    "        mt_config = eval(MotionTagConfigKey)\n",
    "    for filename in fileURLs:\n",
    "        print(filename)\n",
    "        jwt_token = jwt.encode({'sub': 'dumps', 'exp': datetime.datetime.now() + datetime.timedelta(minutes=15)},\n",
    "                            mt_config['mt_secret'], algorithm=\"HS256\")\n",
    "        payload = {'jwt': jwt_token}\n",
    "        r = requests.get(mt_config['mt_base_url'] + filename, params=payload)\n",
    "        with open(path + '\\\\' + filename.split('/')[-1], 'wb') as file:\n",
    "            file.write(r.content)\n",
    "\n",
    "fileURLs = generateWaypointsFilenames(start_date,end_date,offset)\n",
    "downloadData(fileURLs,path_WP_new,MTConfigFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storyline.TUM-ML.2023-11-29--2023-11-29.csv.gz\n",
      "Storyline.TUM-ML.2023-11-30--2023-11-30.csv.gz\n",
      "Storyline.TUM-ML.2023-12-01--2023-12-01.csv.gz\n",
      "Storyline.TUM-ML.2023-12-02--2023-12-02.csv.gz\n",
      "Storyline.TUM-ML.2023-12-03--2023-12-03.csv.gz\n",
      "Storyline.TUM-ML.2023-12-04--2023-12-04.csv.gz\n",
      "Storyline.TUM-ML.2023-12-05--2023-12-05.csv.gz\n",
      "Storyline.TUM-ML.2023-12-06--2023-12-06.csv.gz\n",
      "Storyline.TUM-ML.2023-12-07--2023-12-07.csv.gz\n",
      "Storyline.TUM-ML.2023-12-08--2023-12-08.csv.gz\n",
      "Storyline.TUM-ML.2023-12-09--2023-12-09.csv.gz\n",
      "Storyline.TUM-ML.2023-12-10--2023-12-10.csv.gz\n",
      "Storyline.TUM-ML.2023-12-11--2023-12-11.csv.gz\n",
      "Storyline.TUM-ML.2023-12-12--2023-12-12.csv.gz\n",
      "Storyline.TUM-ML.2023-12-13--2023-12-13.csv.gz\n",
      "Storyline.TUM-ML.2023-12-14--2023-12-14.csv.gz\n",
      "Storyline.TUM-ML.2023-12-15--2023-12-15.csv.gz\n",
      "Storyline.TUM-ML.2023-12-16--2023-12-16.csv.gz\n",
      "Storyline.TUM-ML.2023-12-17--2023-12-17.csv.gz\n",
      "Storyline.TUM-ML.2023-12-18--2023-12-18.csv.gz\n",
      "Storyline.TUM-ML.2023-12-19--2023-12-19.csv.gz\n",
      "Storyline.TUM-ML.2023-12-20--2023-12-20.csv.gz\n",
      "Storyline.TUM-ML.2023-12-21--2023-12-21.csv.gz\n",
      "Storyline.TUM-ML.2023-12-22--2023-12-22.csv.gz\n",
      "Storyline.TUM-ML.2023-12-23--2023-12-23.csv.gz\n",
      "Storyline.TUM-ML.2023-12-24--2023-12-24.csv.gz\n",
      "Storyline.TUM-ML.2023-12-25--2023-12-25.csv.gz\n",
      "Storyline.TUM-ML.2023-12-26--2023-12-26.csv.gz\n",
      "Storyline.TUM-ML.2023-12-27--2023-12-27.csv.gz\n",
      "Storyline.TUM-ML.2023-12-28--2023-12-28.csv.gz\n",
      "Storyline.TUM-ML.2023-12-29--2023-12-29.csv.gz\n",
      "Storyline.TUM-ML.2023-12-30--2023-12-30.csv.gz\n",
      "Storyline.TUM-ML.2023-12-31--2023-12-31.csv.gz\n",
      "Storyline.TUM-ML.2024-01-01--2024-01-01.csv.gz\n",
      "Storyline.TUM-ML.2024-01-02--2024-01-02.csv.gz\n",
      "Storyline.TUM-ML.2024-01-03--2024-01-03.csv.gz\n",
      "Storyline.TUM-ML.2024-01-04--2024-01-04.csv.gz\n",
      "Storyline.TUM-ML.2024-01-05--2024-01-05.csv.gz\n",
      "Storyline.TUM-ML.2024-01-06--2024-01-06.csv.gz\n",
      "Storyline.TUM-ML.2024-01-07--2024-01-07.csv.gz\n",
      "Storyline.TUM-ML.2024-01-08--2024-01-08.csv.gz\n",
      "Storyline.TUM-ML.2024-01-09--2024-01-09.csv.gz\n",
      "Storyline.TUM-ML.2024-01-10--2024-01-10.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# STORYLINE DATA\n",
    "\n",
    "# TODO: set start and end date for the storyline data\n",
    "start_date = pd.to_datetime('2023-11-29') # included \n",
    "end_date = pd.to_datetime('2024-01-10') # included\n",
    "# TODO: number of days per file\n",
    "offset = 1\n",
    "\n",
    "def generateStorylineFilenames(start_date,end_date,offset):\n",
    "    # generate URLs for downloading the files\n",
    "    fileURLs = []\n",
    "    for sd in pd.date_range(start_date, end_date, freq='D'):\n",
    "        # sample file name: Storyline.TUM-ML.2023-06-01--2023-06-01.csv.gz\n",
    "        filename = 'Storyline.TUM-ML.' + str(sd.date()) + '--' + str((sd + pd.DateOffset(days=offset-1)).date()) + '.csv.gz'\n",
    "        start_date = start_date + pd.DateOffset(days=offset)\n",
    "        fileURLs.append(filename)\n",
    "    return fileURLs\n",
    "\n",
    "fileURLs = generateStorylineFilenames(start_date,end_date,offset)\n",
    "downloadData(fileURLs,path_SL_new,MTConfigFile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Convert .gz file to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [path_WP_new,path_SL_new] \n",
    "\n",
    "for path in paths:\n",
    "    done = [f for f in os.listdir(path) if f[-4:]=='.csv']\n",
    "    todo = [f for f in os.listdir(path) if (f[:-3] not in done) and (f[0]!='.') and (f[-3:]=='.gz')]\n",
    "    for filename in todo:\n",
    "        # avoid = '...'\n",
    "        # if avoid in filename:\n",
    "        #     continue\n",
    "        print(filename)\n",
    "        with gzip.open(os.path.join(path,filename), 'rt', newline='') as csv_file:\n",
    "            csv_data = csv_file.read()\n",
    "        newfilename = filename[:-3] # + '.csv'\n",
    "        with open(os.path.join(path,newfilename), 'wt') as out_file:\n",
    "            out_file.write(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Move .gz files to 'waypoints' and 'storylines'\n",
    "\n",
    "The gz files take up much less storage, so store these are a backup, instead of the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [path_WP_new, path_SL_new] \n",
    "paths_for_gz_files = [path_WP_gz, path_SL_gz]  \n",
    "\n",
    "for path,path_for_gz_file in zip(paths,paths_for_gz_files):\n",
    "    for filename in os.listdir(path):\n",
    "        if filename[-3:] == '.gz':\n",
    "            shutil.move(os.path.join(path,filename), os.path.join(path_for_gz_file,filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split waypoints files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0\n",
      "ToDo: 3\n"
     ]
    }
   ],
   "source": [
    "# get all filenames in waypoints folder\n",
    "filenames = [file for file in os.listdir(r'%s'%(path_WP_new))]\n",
    "\n",
    "# distinguish between completed and new files\n",
    "filenames_todo = [file for file in filenames if len(file)>31]\n",
    "filenames_done = [file for file in filenames if len(file)==31]\n",
    "print('Done:',len(filenames_done))\n",
    "print('ToDo:',len(filenames_todo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todo: 3\n",
      "Waypoints.TUM-ML.2024-01-11--2024-01-17.csv\n",
      "2024-01-11\n",
      "2024-01-12\n",
      "2024-01-13\n",
      "2024-01-14\n",
      "2024-01-15\n",
      "2024-01-16\n",
      "2024-01-17\n",
      "Waypoints.TUM-ML.2024-01-08--2024-01-10.csv\n",
      "2024-01-08\n",
      "2024-01-09\n",
      "2024-01-10\n",
      "Waypoints.TUM-ML.2024-01-05--2024-01-07.csv\n",
      "2024-01-05\n",
      "2024-01-06\n",
      "2024-01-07\n"
     ]
    }
   ],
   "source": [
    "completed = [ i[17:-4] for i in filenames_done]\n",
    "# TODO: these columns may (!) differ depending on the waypoints dataset\n",
    "cols = ['user_id', 'tracked_at', 'latitude', 'longitude', 'accuracy', 'speed', 'altitude', 'course']\n",
    "\n",
    "# reversed - to always get the most recent version of a day\n",
    "print('Todo:',len(filenames_todo))\n",
    "for filename in reversed(filenames_todo):\n",
    "    print(filename)\n",
    "    # load file as df\n",
    "    df = pd.read_csv(r'%s\\%s'%(path,filename),sep=';')\n",
    "    # extract date as string\n",
    "    df['date'] = df.tracked_at.apply(lambda x: x[:10])\n",
    "    # remove dates that have already been dealt with\n",
    "    df = df[~df['date'].isin(completed)]\n",
    "    dates = df['date'].unique()\n",
    "    # add dates to list of completed dates\n",
    "    completed.extend(dates)\n",
    "    #save each day as csv\n",
    "    grouped = df.groupby('date')\n",
    "    for date, group in grouped:\n",
    "        print(date)\n",
    "        group.to_csv(f'{path}/Waypoints.TUM-ML.{date}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STOP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that no files (e.g. Waypoints.TUM-ML.2023-02-01.csv) are overwritten i.e. have 1KB size. --> Otherwise rerun the code for the file with the respective day using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the sentence above.\n",
    "\n",
    "# example: \n",
    "#       [[{date}, {corresponding file name}],\\\n",
    "#        ['2022-05-03', 'Waypoints.TUM-ML.2022-05-01--2022-05-07.csv.gz'],\\\n",
    "#        ['2022-07-04', 'Waypoints.TUM-ML.2022-07-02--2022-07-08.csv.gz']] \n",
    "to_redo = [] \n",
    "\n",
    "for filename in [i[1] for i in to_redo]:\n",
    "    print(filename)\n",
    "    # load file as df\n",
    "    df = pd.read_csv(r'%s\\%s'%(path,filename),sep=';')\n",
    "    # extract date as string\n",
    "    df['date'] = df.tracked_at.apply(lambda x: x[:10])\n",
    "    # remove dates that have already been dealt with\n",
    "    df = df[~df['date'].isin(completed)]\n",
    "    dates = df['date'].unique()\n",
    "    # add dates to list of completed dates\n",
    "    completed.extend(dates)\n",
    "    # save each day as csv\n",
    "    grouped = df.groupby('date')\n",
    "    for date, group in grouped:\n",
    "        if date in [i[0] for i in to_redo]:\n",
    "            print(date)\n",
    "            group.to_csv(f'{path}/Waypoints.TUM-ML.{date}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all filenames_todo (e.g., Waypoints.TUM-ML.2023-03-06--2023-03-12)\n",
    "for filename in filenames_todo:\n",
    "    os.remove(os.path.join(path,filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Upload waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to DB\n",
    "if not 'cursor' in locals():\n",
    "    conn,cursor = setUpDBConnection(myDBUsername,myDBPasswordPath)\n",
    "    conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload waypoints directly to the DB\n",
    "\n",
    "filenames = [file for file in os.listdir(r'%s'%(path_WP_new)) if file[-4:]=='.csv']\n",
    "\n",
    "with open(path_WP_new+r'\\waypoints_uploaded.txt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    try:\n",
    "        completed = list(reader)[0]\n",
    "    except:\n",
    "        completed = []\n",
    "\n",
    "filenames_todo = [i for i in filenames if i not in completed]\n",
    "\n",
    "print('Files to be added:')\n",
    "for file in filenames_todo:\n",
    "    print(file)\n",
    "    \n",
    "# send command for each file to copy data to DB\n",
    "for filename in filenames_todo:\n",
    "    print(filename)\n",
    "    try:\n",
    "        copy_command = f'''COPY waypoints(user_id, tracked_at, latitude, longitude, accuracy, speed, altitude, course, date) \n",
    "                FROM STDIN DELIMITER ';' CSV HEADER;'''\n",
    "        cursor.copy_expert(copy_command, open(r'%s\\%s'%(path_WP_new,filename), \"r\"))\n",
    "        print('V1')\n",
    "    except:\n",
    "        copy_command = f'''COPY waypoints(user_id, tracked_at, latitude, longitude, accuracy, speed, altitude, course) \n",
    "                FROM STDIN DELIMITER ',' CSV HEADER;'''\n",
    "        cursor.copy_expert(copy_command, open(r'%s\\%s'%(path_WP_new,filename), \"r\"))\n",
    "        print('V2')\n",
    "    completed.append(filename)\n",
    "    # add date to list of completed dates\n",
    "    with open(path_WP_new+r'\\waypoints_uploaded.txt','w') as f:\n",
    "        f.write(','.join(completed))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all csv files \n",
    "for filename in filenames:\n",
    "    os.remove(os.path.join(path_WP_new,filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Upload storyline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the commands to locally add the individual storyline files to the database. These can be collectively copied and pasted into the psql command window. Pretty fast: approx. 5 seconds per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "len_of_comment = []\n",
    "# also check for all other days till end of november\n",
    "for filename in os.listdir(path_SL_new):\n",
    "    testdf = pd.read_csv(path_SL_new + r\"\\%s\"%(filename), sep=';')\n",
    "    if 'comment_feedback' in testdf.columns:    \n",
    "        # max len and name of last column\n",
    "        try: \n",
    "            max_len = max([len(i) for i in testdf.comment_feedback.unique() if i not in [np.nan,'nan']])\n",
    "        except:\n",
    "            max_len = 0\n",
    "        len_of_comment.append(max_len)\n",
    "print('Max length of comment:',max(len_of_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORYLINE\n",
    "\n",
    "# \\copy storyline(id, user_id, type, started_at, started_at_timezone, finished_at, finished_at_timezone, length, detected_mode, mode, purpose, geometry, confirmed_at, started_on, misdetected_completely, merged, created_at, updated_at,started_at_in_timezone,finished_at_in_timezone,confirmed_at_in_timezone,created_at_in_timezone,updated_at_in_timezone) FROM 'F:\\Storyline.TUM-ML.2022-05-18--2022-05-31.csv' DELIMITER ';'CSV HEADER;\n",
    "\n",
    "for filename in os.listdir(path_SL_new):\n",
    "    if (filename[0]=='S') & (filename[-1]=='v'):\n",
    "        filename = os.path.join(path,filename)\n",
    "        print(\"\\n\\copy storyline(record_id, user_id, type, started_at, started_at_timezone, finished_at, finished_at_timezone, length, detected_mode, mode, purpose, geometry, confirmed_at, started_on, misdetected_completely, merged, created_at, updated_at,started_at_in_timezone,finished_at_in_timezone,confirmed_at_in_timezone,created_at_in_timezone,updated_at_in_timezone, comment_feedback) FROM '%s' DELIMITER ';'CSV HEADER;\"%(filename))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STOP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** make sure to not upload data that has previously been uploaded. If required, delete that data first. Deleting duplicates takes ages... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste the above commands collectively into the \"SQL Shell\" (PSQL command terminal).\n",
    "1. Open 'psql'\n",
    "2. Enter server name: XX.XXX.XX.XXX\n",
    "3. Enter db name: mobilitaetleben\n",
    "4. Enter port: [press enter]\n",
    "5. Enter username: [enter your username]\n",
    "6. Enter password: [enter your password]\n",
    "7. Paste the commands generated in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all csv files \n",
    "for filename in os.listdir(path_SL_new):\n",
    "    os.remove(os.path.join(path_SL_new,filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Run the processing pipeline for the new **storyline** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'cursor' in locals():\n",
    "       conn,cursor = setUpDBConnection(myDBUsername,myDBPasswordPath)\n",
    "\n",
    "# get start and end date\n",
    "manual_entry = False\n",
    "if manual_entry:\n",
    "       end_date = '20221231'\n",
    "else:\n",
    "       cursor.execute(\"SELECT MAX(started_on) FROM storyline;\")\n",
    "       end_date = cursor.fetchall()[0][0].strftime('%Y%m%d')\n",
    "if manual_entry:\n",
    "       start_date = '20220501'\n",
    "else:\n",
    "       cursor.execute(\"SELECT MAX(started_on) FROM storyline_processed;\")\n",
    "       start_date = cursor.fetchall()[0][0].strftime('%Y%m%d')\n",
    "       # set start date to 2 days before because of overlapping trip ids\n",
    "       start_date = pd.to_datetime(start_date) - pd.DateOffset(days=2)\n",
    "       start_date = start_date.strftime('%Y%m%d')\n",
    "# run pipeline.py where args=(query)\n",
    "query = \"\"\"select * from storyline\n",
    "       where (started_on >= '%s') \n",
    "       AND (started_on < '%s');\"\"\"%(start_date,end_date)\n",
    "print(query)\n",
    "\n",
    "from pipeline import *\n",
    "# result = runPipeline(query,write=True,mapMatching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK THAT THE NEW DATES ARE ADDED TO THE DB\n",
    "\n",
    "conn,cursor = setUpDBConnection(myDBUsername,myDBPasswordPath)\n",
    "queryX = \"SELECT started_on FROM storyline_processed;\"\n",
    "cursor.execute(queryX)\n",
    "data = cursor.fetchall()\n",
    "set(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEXT**\n",
    "\n",
    "Run pipeline_subtables.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Make a backup of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyRadeln",
   "language": "python",
   "name": "pyradeln"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4601cce84029fe39ad58aff89ab8406da83ee82e38717570696f8d6030868dc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
